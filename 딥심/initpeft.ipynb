{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: transformers in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from peft) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from peft) (1.0.1)\n",
      "Requirement already satisfied: safetensors in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from peft) (0.25.1)\n",
      "Requirement already satisfied: filelock in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import peft\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timm용\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class MultiClassImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_root_dir, img_column, label_column, img_size=224):\n",
    "        \"\"\"\n",
    "        Multi-class image classification dataset for timm models like MaxVit.\n",
    "\n",
    "        :param csv_file: Path to the CSV file containing image paths and labels.\n",
    "        :param img_root_dir: Root directory containing the images.\n",
    "        :param img_column: Column name in the CSV containing the image file paths.\n",
    "        :param label_column: Column name in the CSV containing the labels.\n",
    "        :param img_size: Target image size (square, e.g., 224x224).\n",
    "        \"\"\"\n",
    "        # Load the CSV file into a DataFrame\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Image paths\n",
    "        self.img_root_dir = img_root_dir\n",
    "        self.img_column = img_column\n",
    "\n",
    "        # Label column\n",
    "        self.label_column = label_column\n",
    "\n",
    "        # Create a label mapping (string to integer)\n",
    "        self.label_mapping = {label: idx for idx, label in enumerate(sorted(self.data[label_column].unique()))}\n",
    "\n",
    "        # Define the image transformations (for MaxVit, use ImageNet normalization)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),  # Resize to target size\n",
    "            transforms.ToTensor(),  # Convert image to tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch an image and its corresponding label by index.\n",
    "\n",
    "        :param idx: Index of the sample.\n",
    "        :return: A dictionary with 'pixel_values' (processed image tensor) and 'label' (integer label).\n",
    "        \"\"\"\n",
    "        # Get the image file path\n",
    "        img_path = os.path.join(self.img_root_dir, self.data.iloc[idx][self.img_column])\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply the transformations\n",
    "        processed_image =self.transform(image)\n",
    "\n",
    "        # Map the label to an integer\n",
    "        label = self.label_mapping[self.data.iloc[idx][self.label_column]]\n",
    "\n",
    "        return {'pixel_values': processed_image, 'label': label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a specific seed value\n",
    "seed_value = 42  # You can choose any integer value\n",
    "set_seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timm용\n",
    "\n",
    "csv_file = \"./train.csv\"\n",
    "img_root_dir = \"./\"\n",
    "# Dataset 인스턴스 생성\n",
    "dataset = MultiClassImageDataset(\n",
    "    csv_file=csv_file,\n",
    "    img_root_dir=img_root_dir,\n",
    "    img_column=\"img_path\",  # CSV 파일의 이미지 경로 열 이름\n",
    "    label_column=\"label\",\n",
    "    #model = model,\n",
    "   # data_config = data_config,# CSV 파일의 라벨 열 이름\n",
    "    img_size=224  # MaxVit 모델의 입력 크기\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idp/lab/song/dl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_path = os.getcwd()\n",
    "print(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "class MaxVitClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super(MaxVitClassifier, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Replace the head with a custom classification layer\n",
    "        in_features = self.base_model.head.in_features\n",
    "        self.base_model.head = nn.Identity()  # Remove original head\n",
    "        self.classifier = nn.Linear(in_features, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.base_model.forward_features(x)\n",
    "        \n",
    "        #print(features.shape)\n",
    "        # Ensure global average pooling is applied correctly\n",
    "        if len(features.shape) == 3:  # [batch_size, seq_len, in_features]\n",
    "            features = features.mean(dim=1)  # Pool across sequence length\n",
    "        elif len(features.shape) == 4:  # [batch_size, in_features, H, W]\n",
    "            features = F.adaptive_avg_pool2d(features, (1, 1)).view(features.size(0), -1)\n",
    "            \n",
    "#\n",
    "        # Pass through classification head\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "    #def forward(self, x):\n",
    "    #    # Extract features\n",
    "    #    features = self.base_model.forward_features(x)\n",
    "#\n",
    "    #    # Permute dimensions to [batch_size, in_features, H, W]\n",
    "    #    features = features.permute(0, 3, 1, 2)\n",
    "#\n",
    "    #    # Apply global average pooling\n",
    "    #    features = F.adaptive_avg_pool2d(features, (1, 1)).view(features.size(0), -1)\n",
    "#\n",
    "    #    # Pass through classification head\n",
    "    #    logits = self.classifier(features)\n",
    "    #    return logits\n",
    "\n",
    "\n",
    "# Step 1: Load the pretrained MaxVit model\n",
    "base_model = timm.create_model('deit3_large_patch16_224.fb_in22k_ft_in1k', pretrained=True)\n",
    "#eva_large_patch14_336.in22k_ft_in22k_in1k, eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\n",
    "#convnext_xxlarge.clip_laion2b_soup_ft_in1k\n",
    "# Step 2: Wrap it in the classifier\n",
    "transform = create_transform(**resolve_data_config(base_model.pretrained_cfg, model=base_model))\n",
    "num_classes = 25\n",
    "model = MaxVitClassifier(base_model, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = create_transform(**resolve_data_config(model.base_model.pretrained_cfg, model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', __main__.MaxVitClassifier),\n",
       " ('base_model', timm.models.vision_transformer.VisionTransformer),\n",
       " ('base_model.patch_embed', timm.layers.patch_embed.PatchEmbed),\n",
       " ('base_model.patch_embed.proj', torch.nn.modules.conv.Conv2d),\n",
       " ('base_model.patch_embed.norm', torch.nn.modules.linear.Identity),\n",
       " ('base_model.pos_drop', torch.nn.modules.dropout.Dropout),\n",
       " ('base_model.patch_drop', torch.nn.modules.linear.Identity),\n",
       " ('base_model.norm_pre', torch.nn.modules.linear.Identity),\n",
       " ('base_model.blocks', torch.nn.modules.container.Sequential),\n",
       " ('base_model.blocks.0', timm.models.vision_transformer.Block),\n",
       " ('base_model.blocks.0.norm1', torch.nn.modules.normalization.LayerNorm),\n",
       " ('base_model.blocks.0.attn', timm.models.vision_transformer.Attention),\n",
       " ('base_model.blocks.0.attn.qkv', torch.nn.modules.linear.Linear),\n",
       " ('base_model.blocks.0.attn.q_norm', torch.nn.modules.linear.Identity),\n",
       " ('base_model.blocks.0.attn.k_norm', torch.nn.modules.linear.Identity),\n",
       " ('base_model.blocks.0.attn.attn_drop', torch.nn.modules.dropout.Dropout),\n",
       " ('base_model.blocks.0.attn.proj', torch.nn.modules.linear.Linear),\n",
       " ('base_model.blocks.0.attn.proj_drop', torch.nn.modules.dropout.Dropout),\n",
       " ('base_model.blocks.0.ls1', timm.models.vision_transformer.LayerScale),\n",
       " ('base_model.blocks.0.drop_path1', torch.nn.modules.linear.Identity),\n",
       " ('base_model.blocks.0.norm2', torch.nn.modules.normalization.LayerNorm),\n",
       " ('base_model.blocks.0.mlp', timm.layers.mlp.Mlp),\n",
       " ('base_model.blocks.0.mlp.fc1', torch.nn.modules.linear.Linear),\n",
       " ('base_model.blocks.0.mlp.act', torch.nn.modules.activation.GELU),\n",
       " ('base_model.blocks.0.mlp.drop1', torch.nn.modules.dropout.Dropout),\n",
       " ('base_model.blocks.0.mlp.norm', torch.nn.modules.linear.Identity),\n",
       " ('base_model.blocks.0.mlp.fc2', torch.nn.modules.linear.Linear),\n",
       " ('base_model.blocks.0.mlp.drop2', torch.nn.modules.dropout.Dropout),\n",
       " ('base_model.blocks.0.ls2', timm.models.vision_transformer.LayerScale),\n",
       " ('base_model.blocks.0.drop_path2', torch.nn.modules.linear.Identity)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(n, type(m)) for n, m in model.named_modules()][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('base_model.norm', torch.nn.modules.normalization.LayerNorm),\n",
       " ('base_model.fc_norm', torch.nn.modules.linear.Identity),\n",
       " ('base_model.head_drop', torch.nn.modules.dropout.Dropout),\n",
       " ('base_model.head', torch.nn.modules.linear.Identity),\n",
       " ('classifier', torch.nn.modules.linear.Linear)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(n, type(m)) for n, m in model.named_modules()][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = peft.LoraConfig(r=8, target_modules=r\".*\\.mlp\\.fc\\d\", modules_to_save=[\"classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,991,705 || all params: 305,367,090 || trainable%: 0.6522\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "peft_model = peft.get_peft_model(model, config).to(device)\n",
    "optimizer = torch.optim.Adam(peft_model.parameters(), lr=2e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n",
      "    service.run()\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 98, in run\n",
      "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 115, in login\n",
      "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 191, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/codecs.py\", line 319, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login\n",
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d575dca60db478689f2272e7edc1a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/2, Training Loss: 0.008854657171018562\n",
      "Epoch 2/2, Training Loss: 0.0029064079967634146\n",
      "Validation Loss: 0.0027, Accuracy: 1.0000\n",
      "Fold 2/5\n",
      "Epoch 1/2, Training Loss: 0.003250337196503193\n",
      "Epoch 2/2, Training Loss: 0.0026265482694169703\n",
      "Validation Loss: 0.0037, Accuracy: 0.9994\n",
      "Fold 3/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     49\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 51\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "num_epochs = 2\n",
    "batch_size = 16\n",
    "learning_rate = 5e-5\n",
    "num_classes = 10\n",
    "# Define criterion and device\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# K-Fold Training\n",
    "fold_results = {}\n",
    "#data_config = timm.data.resolve_model_data_config(peft_model)\n",
    "#transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold + 1}/{kf.n_splits}\")\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(peft_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        peft_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            #outputs = model(pixel_values=pixel_values)\n",
    "            #pixel_values = pixel_values.squeeze(0)\n",
    "            outputs = peft_model(pixel_values)\n",
    "            logits = outputs\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation loop\n",
    "    peft_model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = peft_model(pixel_values)\n",
    "            logits = outputs\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Save results for the fold\n",
    "    fold_results[fold] = {'val_loss': val_loss, 'accuracy': accuracy}\n",
    "\n",
    "# Print overall results\n",
    "print(\"K-Fold Cross Validation Results:\")\n",
    "for fold, result in fold_results.items():\n",
    "    print(f\"Fold {fold + 1}: Loss: {result['val_loss']:.4f}, Accuracy: {result['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(peft_model, f'peftmodel_fold_epoch4_1344.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98623/3963252572.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  peft_model = torch.load('peftmodel_fold_epoch4_1344.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MaxVitClassifier(\n",
       "      (base_model): VisionTransformer(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "          (norm): Identity()\n",
       "        )\n",
       "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "        (patch_drop): Identity()\n",
       "        (norm_pre): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (10): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (11): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (12): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (13): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (14): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (15): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (16): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (17): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (18): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (19): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (20): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (21): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (22): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (23): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc_norm): Identity()\n",
       "        (head_drop): Dropout(p=0.0, inplace=False)\n",
       "        (head): Identity()\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=1024, out_features=25, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=1024, out_features=25, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model = torch.load('peftmodel_fold_epoch4_1344.pth')\n",
    "peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 23.54 GiB of which 96.81 MiB is free. Process 98638 has 11.87 GiB memory in use. Including non-PyTorch memory, this process has 11.55 GiB memory in use. Of the allocated memory 10.90 GiB is allocated by PyTorch, and 204.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 48\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     51\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/mushroom2/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mushroom2/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mushroom2/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 23.54 GiB of which 96.81 MiB is free. Process 98638 has 11.87 GiB memory in use. Including non-PyTorch memory, this process has 11.55 GiB memory in use. Of the allocated memory 10.90 GiB is allocated by PyTorch, and 204.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# Define criterion and device\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# K-Fold Training\n",
    "fold_results = {}\n",
    "#data_config = timm.data.resolve_model_data_config(peft_model)\n",
    "#transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold + 1}/{kf.n_splits}\")\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(peft_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        peft_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            #outputs = model(pixel_values=pixel_values)\n",
    "            #pixel_values = pixel_values.squeeze(0)\n",
    "            outputs = peft_model(pixel_values)\n",
    "            logits = outputs\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation loop\n",
    "    peft_model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = peft_model(pixel_values)\n",
    "            logits = outputs\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Save results for the fold\n",
    "    fold_results[fold] = {'val_loss': val_loss, 'accuracy': accuracy}\n",
    "\n",
    "# Print overall results\n",
    "print(\"K-Fold Cross Validation Results:\")\n",
    "for fold, result in fold_results.items():\n",
    "    print(f\"Fold {fold + 1}: Loss: {result['val_loss']:.4f}, Accuracy: {result['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "그대로\n",
    "Fold 1/5\n",
    "Epoch 1/1, Training Loss: 0.004091352261509495\n",
    "Validation Loss: 0.0027, Accuracy: 0.9997\n",
    "Fold 2/5\n",
    "Epoch 1/1, Training Loss: 0.0016328931443981697\n",
    "Validation Loss: 0.0010, Accuracy: 1.0000\n",
    "Fold 3/5\n",
    "Epoch 1/1, Training Loss: 0.0023818855987527733\n",
    "Validation Loss: 0.0024, Accuracy: 0.9997\n",
    "Fold 4/5\n",
    "Epoch 1/1, Training Loss: 0.0008638568190824562\n",
    "Validation Loss: 0.0003, Accuracy: 1.0000\n",
    "Fold 5/5\n",
    "Epoch 1/1, Training Loss: 0.0030112070329319164\n",
    "Validation Loss: 0.0013, Accuracy: 1.0000\n",
    "K-Fold Cross Validation Results:\n",
    "Fold 1: Loss: 0.0027, Accuracy: 0.9997\n",
    "Fold 2: Loss: 0.0010, Accuracy: 1.0000\n",
    "Fold 3: Loss: 0.0024, Accuracy: 0.9997\n",
    "Fold 4: Loss: 0.0003, Accuracy: 1.0000\n",
    "Fold 5: Loss: 0.0013, Accuracy: 1.0000\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 5e-4\n",
    "\n",
    "Fold 1/5\n",
    "Epoch 1/1, Training Loss: 0.0694166306278232\n",
    "Validation Loss: 0.0896, Accuracy: 0.9747\n",
    "Fold 2/5\n",
    "Epoch 1/1, Training Loss: 0.05943770932480211\n",
    "Validation Loss: 0.0509, Accuracy: 0.9836\n",
    "Fold 3/5\n",
    "Epoch 1/1, Training Loss: 0.03913040896543981\n",
    "Validation Loss: 0.0266, Accuracy: 0.9908\n",
    "Fold 4/5\n",
    "Epoch 1/1, Training Loss: 0.02210452046196536\n",
    "Validation Loss: 0.0220, Accuracy: 0.9927\n",
    "Fold 5/5\n",
    "Epoch 1/1, Training Loss: 0.014862824712436535\n",
    "Validation Loss: 0.0160, Accuracy: 0.9953\n",
    "K-Fold Cross Validation Results:\n",
    "Fold 1: Loss: 0.0896, Accuracy: 0.9747\n",
    "Fold 2: Loss: 0.0509, Accuracy: 0.9836\n",
    "Fold 3: Loss: 0.0266, Accuracy: 0.9908\n",
    "Fold 4: Loss: 0.0220, Accuracy: 0.9927\n",
    "Fold 5: Loss: 0.0160, Accuracy: 0.9953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mamba\n",
    "Fold 1/5\n",
    "Epoch 1/2, Training Loss: 0.4465740251741748\n",
    "Epoch 2/2, Training Loss: 0.16250977216553322\n",
    "Validation Loss: 0.3011, Accuracy: 0.9132\n",
    "Fold 2/5\n",
    "Epoch 1/2, Training Loss: 0.1834138295777269\n",
    "Epoch 2/2, Training Loss: 0.1016049177207716\n",
    "Validation Loss: 0.1836, Accuracy: 0.9438\n",
    "Fold 3/5\n",
    "Epoch 1/2, Training Loss: 0.11439399200452215\n",
    "Epoch 2/2, Training Loss: 0.08508667345356088\n",
    "Validation Loss: 0.0977, Accuracy: 0.9703\n",
    "Fold 4/5\n",
    "Epoch 1/2, Training Loss: 0.0833165806720945\n",
    "Epoch 2/2, Training Loss: 0.05709977714797439\n",
    "Validation Loss: 0.0614, Accuracy: 0.9814\n",
    "Fold 5/5\n",
    "Epoch 1/2, Training Loss: 0.0755088838690371\n",
    "Epoch 2/2, Training Loss: 0.04305739817414408\n",
    "Validation Loss: 0.1002, Accuracy: 0.9719\n",
    "K-Fold Cross Validation Results:\n",
    "Fold 1: Loss: 0.3011, Accuracy: 0.9132\n",
    "Fold 2: Loss: 0.1836, Accuracy: 0.9438\n",
    "Fold 3: Loss: 0.0977, Accuracy: 0.9703\n",
    "Fold 4: Loss: 0.0614, Accuracy: 0.9814\n",
    "Fold 5: Loss: 0.1002, Accuracy: 0.9719\n",
    "\n",
    "convmini\n",
    "Fold 1/5\n",
    "Epoch 1/2, Training Loss: 0.487710864880361\n",
    "Epoch 2/2, Training Loss: 0.11407356076256521\n",
    "Validation Loss: 0.2265, Accuracy: 0.9324\n",
    "Fold 2/5\n",
    "Epoch 1/2, Training Loss: 0.12080685623134217\n",
    "Epoch 2/2, Training Loss: 0.05445691198797065\n",
    "Validation Loss: 0.0731, Accuracy: 0.9751\n",
    "Fold 3/5\n",
    "Epoch 1/2, Training Loss: 0.0641969392777401\n",
    "Epoch 2/2, Training Loss: 0.033054238663262736\n",
    "Validation Loss: 0.0308, Accuracy: 0.9896\n",
    "Fold 4/5\n",
    "Epoch 1/2, Training Loss: 0.0461322780592356\n",
    "Epoch 2/2, Training Loss: 0.026519651799774675\n",
    "Validation Loss: 0.0240, Accuracy: 0.9915\n",
    "Fold 5/5\n",
    "Epoch 1/2, Training Loss: 0.039437890470422864\n",
    "Epoch 2/2, Training Loss: 0.02805027757881362\n",
    "Validation Loss: 0.0445, Accuracy: 0.9867\n",
    "K-Fold Cross Validation Results:\n",
    "Fold 1: Loss: 0.2265, Accuracy: 0.9324\n",
    "Fold 2: Loss: 0.0731, Accuracy: 0.9751\n",
    "Fold 3: Loss: 0.0308, Accuracy: 0.9896\n",
    "Fold 4: Loss: 0.0240, Accuracy: 0.9915\n",
    "Fold 5: Loss: 0.0445, Accuracy: 0.9867\n",
    "\n",
    "peft\n",
    "Fold 1/5\n",
    "Epoch 1/2, Training Loss: 1.253221397043554\n",
    "Epoch 2/2, Training Loss: 0.1957473995133463\n",
    "Validation Loss: 0.1787, Accuracy: 0.9495\n",
    "Fold 2/5\n",
    "Epoch 1/2, Training Loss: 0.12313004221909472\n",
    "Epoch 2/2, Training Loss: 0.07577837749564846\n",
    "Validation Loss: 0.0993, Accuracy: 0.9706\n",
    "Fold 3/5\n",
    "Epoch 1/2, Training Loss: 0.06342431691709954\n",
    "Epoch 2/2, Training Loss: 0.035829081823076286\n",
    "Validation Loss: 0.0386, Accuracy: 0.9905\n",
    "Fold 4/5\n",
    "Epoch 1/2, Training Loss: 0.029754114269117695\n",
    "Epoch 2/2, Training Loss: 0.014582787669221007\n",
    "Validation Loss: 0.0188, Accuracy: 0.9959\n",
    "Fold 5/5\n",
    "Epoch 1/2, Training Loss: 0.013216604322527322\n",
    "Epoch 2/2, Training Loss: 0.005126675787975884\n",
    "Validation Loss: 0.0076, Accuracy: 0.9984\n",
    "K-Fold Cross Validation Results:\n",
    "Fold 1: Loss: 0.1787, Accuracy: 0.9495\n",
    "Fold 2: Loss: 0.0993, Accuracy: 0.9706\n",
    "Fold 3: Loss: 0.0386, Accuracy: 0.9905\n",
    "Fold 4: Loss: 0.0188, Accuracy: 0.9959\n",
    "Fold 5: Loss: 0.0076, Accuracy: 0.9984\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision import transforms\n",
    "#timm 용\n",
    "\n",
    "class MultiClassImageDatasetTest(Dataset):\n",
    "    def __init__(self, csv_file, img_root_dir, img_column,img_size=224):\n",
    "        \"\"\"\n",
    "        Multi-class image classification dataset for inference.\n",
    "\n",
    "        :param csv_file: Path to the CSV file containing image paths.\n",
    "        :param img_root_dir: Root directory containing the images.\n",
    "        :param img_column: Column name in the CSV containing the image file paths.\n",
    "        :param processor: Hugging Face AutoImageProcessor for image preprocessing.\n",
    "        \"\"\"\n",
    "        # Load the CSV file into a DataFrame\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Image paths\n",
    "        self.img_root_dir = img_root_dir\n",
    "        self.img_column = img_column\n",
    "\n",
    "        # Define the image transformations (for MaxVit, use ImageNet normalization)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),  # Resize to target size\n",
    "            transforms.ToTensor(),  # Convert image to tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch an image by index and preprocess it.\n",
    "\n",
    "        :param idx: Index of the sample.\n",
    "        :return: A dictionary with 'pixel_values' (processed image tensor).\n",
    "        \"\"\"\n",
    "        # Get the image file path\n",
    "        img_path = os.path.join(self.img_root_dir, self.data.iloc[idx][self.img_column])\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply processor transforms\n",
    "        processed_image =self.transform(image)\n",
    "        return {'pixel_values': processed_image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timm용\n",
    "\n",
    "test_csv_file = \"./test.csv\"\n",
    "img_root_dir = \"./\"\n",
    "img_column = \"img_path\"\n",
    "\n",
    "test_dataset = MultiClassImageDatasetTest(\n",
    "    csv_file=test_csv_file,\n",
    "    img_root_dir=img_root_dir,\n",
    "    img_column=img_column\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#timm용\n",
    "def save_predictions_to_csv_huggingface(model, dataloader, dataset2, submission_file, output_file, device='cpu'):\n",
    "    \"\"\"\n",
    "    Perform inference on a DataLoader using a Hugging Face model and save the results to a CSV file.\n",
    "\n",
    "    :param model: Hugging Face model for image classification.\n",
    "    :param dataloader: DataLoader containing the test dataset.\n",
    "    :param dataset: Dataset object that contains label_mapping.\n",
    "    :param submission_file: Path to the submission CSV file to update.\n",
    "    :param output_file: Path to save the updated submission file.\n",
    "    :param device: Device to perform inference on ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "\n",
    "    # Use the label mapping from the dataset\n",
    "    label_mapping = {v: k for k, v in dataset.label_mapping.items()}  # Reverse mapping for inference\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['pixel_values'].to(device)\n",
    "            outputs = model(images)\n",
    "            logits = outputs\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class_indices = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "            # Map predicted indices to class names\n",
    "            for idx in predicted_class_indices:\n",
    "                predicted_class = label_mapping[idx.item()]\n",
    "                predictions.append(predicted_class)\n",
    "\n",
    "    # Load the submission file\n",
    "    submission_df = pd.read_csv(submission_file)\n",
    "\n",
    "    # Update the 'label' column with predictions\n",
    "    submission_df['label'] = predictions\n",
    "\n",
    "    # Save the updated submission file\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission_with_predictions_peft_fold2.csv\n"
     ]
    }
   ],
   "source": [
    "# 추론 및 저장\n",
    "save_predictions_to_csv_huggingface(\n",
    "    model=peft_model,\n",
    "    dataloader=test_dataloader,\n",
    "    dataset2=dataset,  # 데이터셋 객체 전달\n",
    "    submission_file=\"sample_submission.csv\",\n",
    "    output_file=\"submission_with_predictions_peft_fold2.csv\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mushroom2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
