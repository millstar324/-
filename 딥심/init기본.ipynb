{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timm용\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class MultiClassImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_root_dir, img_column, label_column, img_size=224):\n",
    "        \"\"\"\n",
    "        Multi-class image classification dataset for timm models like MaxVit.\n",
    "\n",
    "        :param csv_file: Path to the CSV file containing image paths and labels.\n",
    "        :param img_root_dir: Root directory containing the images.\n",
    "        :param img_column: Column name in the CSV containing the image file paths.\n",
    "        :param label_column: Column name in the CSV containing the labels.\n",
    "        :param img_size: Target image size (square, e.g., 224x224).\n",
    "        \"\"\"\n",
    "        # Load the CSV file into a DataFrame\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Image paths\n",
    "        self.img_root_dir = img_root_dir\n",
    "        self.img_column = img_column\n",
    "\n",
    "        # Label column\n",
    "        self.label_column = label_column\n",
    "\n",
    "        # Create a label mapping (string to integer)\n",
    "        self.label_mapping = {label: idx for idx, label in enumerate(sorted(self.data[label_column].unique()))}\n",
    "\n",
    "        # Define the image transformations (for MaxVit, use ImageNet normalization)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),  # Resize to target size\n",
    "            transforms.ToTensor(),  # Convert image to tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch an image and its corresponding label by index.\n",
    "\n",
    "        :param idx: Index of the sample.\n",
    "        :return: A dictionary with 'pixel_values' (processed image tensor) and 'label' (integer label).\n",
    "        \"\"\"\n",
    "        # Get the image file path\n",
    "        img_path = os.path.join(self.img_root_dir, self.data.iloc[idx][self.img_column])\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply the transformations\n",
    "        processed_image =self.transform(image)\n",
    "\n",
    "        # Map the label to an integer\n",
    "        label = self.label_mapping[self.data.iloc[idx][self.label_column]]\n",
    "\n",
    "        return {'pixel_values': processed_image, 'label': label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a specific seed value\n",
    "seed_value = 42  # You can choose any integer value\n",
    "set_seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timm용\n",
    "\n",
    "csv_file = \"./train.csv\"\n",
    "img_root_dir = \"./\"\n",
    "# Dataset 인스턴스 생성\n",
    "dataset = MultiClassImageDataset(\n",
    "    csv_file=csv_file,\n",
    "    img_root_dir=img_root_dir,\n",
    "    img_column=\"img_path\",  # CSV 파일의 이미지 경로 열 이름\n",
    "    label_column=\"label\",\n",
    "    #model = model,\n",
    "   # data_config = data_config,# CSV 파일의 라벨 열 이름\n",
    "    img_size=224  # MaxVit 모델의 입력 크기\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idp/lab/song/dl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_path = os.getcwd()\n",
    "print(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "class MaxVitClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super(MaxVitClassifier, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Replace the head with a custom classification layer\n",
    "        in_features = self.base_model.head.in_features\n",
    "        self.base_model.head = nn.Identity()  # Remove original head\n",
    "        self.classifier = nn.Linear(in_features, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.base_model.forward_features(x)\n",
    "        \n",
    "        #print(features.shape)\n",
    "        # Ensure global average pooling is applied correctly\n",
    "        if len(features.shape) == 3:  # [batch_size, seq_len, in_features]\n",
    "            features = features.mean(dim=1)  # Pool across sequence length\n",
    "        elif len(features.shape) == 4:  # [batch_size, in_features, H, W]\n",
    "            features = F.adaptive_avg_pool2d(features, (1, 1)).view(features.size(0), -1)\n",
    "            \n",
    "#\n",
    "        # Pass through classification head\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "    #def forward(self, x):\n",
    "    #    # Extract features\n",
    "    #    features = self.base_model.forward_features(x)\n",
    "#\n",
    "    #    # Permute dimensions to [batch_size, in_features, H, W]\n",
    "    #    features = features.permute(0, 3, 1, 2)\n",
    "#\n",
    "    #    # Apply global average pooling\n",
    "    #    features = F.adaptive_avg_pool2d(features, (1, 1)).view(features.size(0), -1)\n",
    "#\n",
    "    #    # Pass through classification head\n",
    "    #    logits = self.classifier(features)\n",
    "    #    return logits\n",
    "\n",
    "\n",
    "# Step 1: Load the pretrained MaxVit model\n",
    "base_model = timm.create_model('deit3_large_patch16_224.fb_in22k_ft_in1k', pretrained=True)\n",
    "#eva_large_patch14_336.in22k_ft_in22k_in1k, eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\n",
    "#convnext_xxlarge.clip_laion2b_soup_ft_in1k\n",
    "# Step 2: Wrap it in the classifier\n",
    "num_classes = 25\n",
    "model = MaxVitClassifier(base_model, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n",
      "    service.run()\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 98, in run\n",
      "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 115, in login\n",
      "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 191, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "  File \"/home/idp/anaconda3/envs/mushroom2/lib/python3.10/codecs.py\", line 319, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login\n",
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d575dca60db478689f2272e7edc1a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/2, Training Loss: 0.3202856825563217\n",
      "Epoch 2/2, Training Loss: 0.05507965279940261\n",
      "Validation Loss: 0.2269, Accuracy: 0.9403\n",
      "Fold 2/5\n",
      "Epoch 1/2, Training Loss: 0.08218301670958589\n",
      "Epoch 2/2, Training Loss: 0.03371131113188156\n",
      "Validation Loss: 0.0633, Accuracy: 0.9785\n",
      "Fold 3/5\n",
      "Epoch 1/2, Training Loss: 0.04426032769037183\n",
      "Epoch 2/2, Training Loss: 0.031119451142427406\n",
      "Validation Loss: 0.0289, Accuracy: 0.9915\n",
      "Fold 4/5\n",
      "Epoch 1/2, Training Loss: 0.03269628165681485\n",
      "Epoch 2/2, Training Loss: 0.027161028096550195\n",
      "Validation Loss: 0.0143, Accuracy: 0.9953\n",
      "Fold 5/5\n",
      "Epoch 1/2, Training Loss: 0.029864186395190763\n",
      "Epoch 2/2, Training Loss: 0.010032064230340254\n",
      "Validation Loss: 0.0101, Accuracy: 0.9978\n",
      "K-Fold Cross Validation Results:\n",
      "Fold 1: Loss: 0.2269, Accuracy: 0.9403\n",
      "Fold 2: Loss: 0.0633, Accuracy: 0.9785\n",
      "Fold 3: Loss: 0.0289, Accuracy: 0.9915\n",
      "Fold 4: Loss: 0.0143, Accuracy: 0.9953\n",
      "Fold 5: Loss: 0.0101, Accuracy: 0.9978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "num_epochs = 2\n",
    "batch_size = 16\n",
    "learning_rate = 5e-5\n",
    "num_classes = 10\n",
    "# Define criterion and device\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# K-Fold Training\n",
    "fold_results = {}\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold + 1}/{kf.n_splits}\")\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            #outputs = model(pixel_values=pixel_values)\n",
    "            #pixel_values = pixel_values.squeeze(0)\n",
    "            outputs = model(pixel_values)\n",
    "            logits = outputs\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values)\n",
    "            logits = outputs\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Save results for the fold\n",
    "    fold_results[fold] = {'val_loss': val_loss, 'accuracy': accuracy}\n",
    "\n",
    "# Print overall results\n",
    "print(\"K-Fold Cross Validation Results:\")\n",
    "for fold, result in fold_results.items():\n",
    "    print(f\"Fold {fold + 1}: Loss: {result['val_loss']:.4f}, Accuracy: {result['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'model_fold_epoch2_1245.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mamba\n",
    "Fold 1/5\n",
    "Epoch 1/2, Training Loss: 0.4465740251741748\n",
    "Epoch 2/2, Training Loss: 0.16250977216553322\n",
    "Validation Loss: 0.3011, Accuracy: 0.9132\n",
    "Fold 2/5\n",
    "Epoch 1/2, Training Loss: 0.1834138295777269\n",
    "Epoch 2/2, Training Loss: 0.1016049177207716\n",
    "Validation Loss: 0.1836, Accuracy: 0.9438\n",
    "Fold 3/5\n",
    "Epoch 1/2, Training Loss: 0.11439399200452215\n",
    "Epoch 2/2, Training Loss: 0.08508667345356088\n",
    "Validation Loss: 0.0977, Accuracy: 0.9703\n",
    "Fold 4/5\n",
    "Epoch 1/2, Training Loss: 0.0833165806720945\n",
    "Epoch 2/2, Training Loss: 0.05709977714797439\n",
    "Validation Loss: 0.0614, Accuracy: 0.9814\n",
    "Fold 5/5\n",
    "Epoch 1/2, Training Loss: 0.0755088838690371\n",
    "Epoch 2/2, Training Loss: 0.04305739817414408\n",
    "Validation Loss: 0.1002, Accuracy: 0.9719\n",
    "K-Fold Cross Validation Results:\n",
    "Fold 1: Loss: 0.3011, Accuracy: 0.9132\n",
    "Fold 2: Loss: 0.1836, Accuracy: 0.9438\n",
    "Fold 3: Loss: 0.0977, Accuracy: 0.9703\n",
    "Fold 4: Loss: 0.0614, Accuracy: 0.9814\n",
    "Fold 5: Loss: 0.1002, Accuracy: 0.9719\n",
    "\n",
    "convmini\n",
    "Fold 1/5\n",
    "Epoch 1/2, Training Loss: 0.487710864880361\n",
    "Epoch 2/2, Training Loss: 0.11407356076256521\n",
    "Validation Loss: 0.2265, Accuracy: 0.9324\n",
    "Fold 2/5\n",
    "Epoch 1/2, Training Loss: 0.12080685623134217\n",
    "Epoch 2/2, Training Loss: 0.05445691198797065\n",
    "Validation Loss: 0.0731, Accuracy: 0.9751\n",
    "Fold 3/5\n",
    "Epoch 1/2, Training Loss: 0.0641969392777401\n",
    "Epoch 2/2, Training Loss: 0.033054238663262736\n",
    "Validation Loss: 0.0308, Accuracy: 0.9896\n",
    "Fold 4/5\n",
    "Epoch 1/2, Training Loss: 0.0461322780592356\n",
    "Epoch 2/2, Training Loss: 0.026519651799774675\n",
    "Validation Loss: 0.0240, Accuracy: 0.9915\n",
    "Fold 5/5\n",
    "Epoch 1/2, Training Loss: 0.039437890470422864\n",
    "Epoch 2/2, Training Loss: 0.02805027757881362\n",
    "Validation Loss: 0.0445, Accuracy: 0.9867\n",
    "K-Fold Cross Validation Results:\n",
    "Fold 1: Loss: 0.2265, Accuracy: 0.9324\n",
    "Fold 2: Loss: 0.0731, Accuracy: 0.9751\n",
    "Fold 3: Loss: 0.0308, Accuracy: 0.9896\n",
    "Fold 4: Loss: 0.0240, Accuracy: 0.9915\n",
    "Fold 5: Loss: 0.0445, Accuracy: 0.9867\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision import transforms\n",
    "#timm 용\n",
    "\n",
    "class MultiClassImageDatasetTest(Dataset):\n",
    "    def __init__(self, csv_file, img_root_dir, img_column,img_size=384):\n",
    "        \"\"\"\n",
    "        Multi-class image classification dataset for inference.\n",
    "\n",
    "        :param csv_file: Path to the CSV file containing image paths.\n",
    "        :param img_root_dir: Root directory containing the images.\n",
    "        :param img_column: Column name in the CSV containing the image file paths.\n",
    "        :param processor: Hugging Face AutoImageProcessor for image preprocessing.\n",
    "        \"\"\"\n",
    "        # Load the CSV file into a DataFrame\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Image paths\n",
    "        self.img_root_dir = img_root_dir\n",
    "        self.img_column = img_column\n",
    "\n",
    "        # Define the image transformations (for MaxVit, use ImageNet normalization)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),  # Resize to target size\n",
    "            transforms.ToTensor(),  # Convert image to tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch an image by index and preprocess it.\n",
    "\n",
    "        :param idx: Index of the sample.\n",
    "        :return: A dictionary with 'pixel_values' (processed image tensor).\n",
    "        \"\"\"\n",
    "        # Get the image file path\n",
    "        img_path = os.path.join(self.img_root_dir, self.data.iloc[idx][self.img_column])\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply processor transforms\n",
    "        processed_image =self.transform(image)\n",
    "        return {'pixel_values': processed_image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timm용\n",
    "\n",
    "test_csv_file = \"./test.csv\"\n",
    "img_root_dir = \"./\"\n",
    "img_column = \"img_path\"\n",
    "\n",
    "test_dataset = MultiClassImageDatasetTest(\n",
    "    csv_file=test_csv_file,\n",
    "    img_root_dir=img_root_dir,\n",
    "    img_column=img_column\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#timm용\n",
    "def save_predictions_to_csv_huggingface(model, dataloader, dataset2, submission_file, output_file, device='cpu'):\n",
    "    \"\"\"\n",
    "    Perform inference on a DataLoader using a Hugging Face model and save the results to a CSV file.\n",
    "\n",
    "    :param model: Hugging Face model for image classification.\n",
    "    :param dataloader: DataLoader containing the test dataset.\n",
    "    :param dataset: Dataset object that contains label_mapping.\n",
    "    :param submission_file: Path to the submission CSV file to update.\n",
    "    :param output_file: Path to save the updated submission file.\n",
    "    :param device: Device to perform inference on ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "\n",
    "    # Use the label mapping from the dataset\n",
    "    label_mapping = {v: k for k, v in dataset.label_mapping.items()}  # Reverse mapping for inference\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['pixel_values'].to(device)\n",
    "            outputs = model(images)\n",
    "            logits = outputs\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class_indices = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "            # Map predicted indices to class names\n",
    "            for idx in predicted_class_indices:\n",
    "                predicted_class = label_mapping[idx.item()]\n",
    "                predictions.append(predicted_class)\n",
    "\n",
    "    # Load the submission file\n",
    "    submission_df = pd.read_csv(submission_file)\n",
    "\n",
    "    # Update the 'label' column with predictions\n",
    "    submission_df['label'] = predictions\n",
    "\n",
    "    # Save the updated submission file\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# 추론 및 저장\n",
    "save_predictions_to_csv_huggingface(\n",
    "    model=model,\n",
    "    dataloader=test_dataloader,\n",
    "    dataset2=dataset,  # 데이터셋 객체 전달\n",
    "    submission_file=\"sample_submission.csv\",\n",
    "    output_file=\"submission_with_predictions.csv\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mushroom2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
